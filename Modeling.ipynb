{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCPcKQSeqyyv"
      },
      "outputs": [],
      "source": [
        "# A few imports to make everything work\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import cartopy.crs as ccrs\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "import sys\n",
        "from IPython.display import HTML\n",
        "from libpysal.weights import Queen\n",
        "from esda.moran import Moran\n",
        "from scipy.stats import invgamma\n",
        "import copy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import arviz as az\n",
        "from cmdstanpy import CmdStanModel, install_cmdstan\n",
        "\n",
        "from helper_functions import standardize_columns, filter_dataframe_by_date_and_columns, process_and_add_deltas, fill_valore_array, fill_delta_array, calculate_distance_matrix, calculate_distance_matrix_pred, save_stan_fit_and_summary_with_data\n",
        "\n",
        "# Getting rid of a few warnings (can be toggled off)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvtuedgwyofP"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade cmdstanpy arviz\n",
        "\n",
        "# Create ./stan folder if does not exists\n",
        "if not os.path.exists(\"./stan\"):\n",
        "    os.mkdir(\"./stan\")\n",
        "\n",
        "%conda install cmdstan -c conda-forge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMjfpT1y2WJH"
      },
      "outputs": [],
      "source": [
        "url=\"https://raw.githubusercontent.com/fmtibaa/Bayesian_Project_A1/main/content/final.csv\"\n",
        "df_final=pd.read_csv(url,low_memory=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pshl-kb0FPz"
      },
      "source": [
        "### Implementing STAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE_fWMZiyz81"
      },
      "outputs": [],
      "source": [
        "spt_lm = \\\n",
        "\"\"\"\n",
        "data {\n",
        "    int<lower=0> N_years;\n",
        "    int<lower=0> N_weeks;\n",
        "    int<lower=0> N_stations;\n",
        "    int<lower=0> N_covariates;\n",
        "\n",
        "    int<lower=0> N_years_to_pred;\n",
        "    int<lower=0> N_weeks_to_pred;\n",
        "    int<lower=0> N_stations_to_pred;\n",
        "\n",
        "    array[N_years,N_weeks,N_stations] real y;\n",
        "    array[N_years,N_weeks] matrix[N_stations,N_covariates] delta;\n",
        "    matrix[N_stations,N_stations] dist; //distance matrix\n",
        "\n",
        "    matrix[N_stations_to_pred, N_stations] dist_pred; // distance matrix which represents the distance from the stations to predict to every other station in the data set\n",
        "    matrix[N_stations_to_pred, N_stations_to_pred] dist_pred_to_pred; // distance matrix which represents the distance between the stations to predict\n",
        "    vector[N_weeks_to_pred] weeks_to_pred; // represents the time on which we want to perform prediction\n",
        "\n",
        "    real<lower=0> a;\n",
        "    real<lower=0> b;\n",
        "    real<lower=0> s0;\n",
        "    real<lower=0> phi_gamma_esti;\n",
        "    real<lower=0> phi_eta_esti;\n",
        "}\n",
        "\n",
        "parameters {\n",
        "    real<lower=0,upper=1> rho;\n",
        "    array[N_years] real xi;\n",
        "    array[N_covariates] real beta;\n",
        "    array[N_years,N_weeks,N_stations] real eta;\n",
        "    array[N_years] real mu;\n",
        "    array[N_years,N_stations] real gamma;\n",
        "\n",
        "    matrix[N_covariates, N_covariates] A_raw;  // Lower triangular matrix elements (raw scale)\n",
        "    array[N_covariates] real phi_delta;\n",
        "\n",
        "    real<lower=0> tau_eta;\n",
        "    real<lower=0> tau_gamma;\n",
        "    real<lower=0> tau_epsilon;\n",
        "}\n",
        "\n",
        "transformed parameters {\n",
        "    matrix[N_stations,N_stations] Sigma_eta;\n",
        "    matrix[N_stations,N_stations] Sigma_gamma;\n",
        "    for (i in 1:N_stations){\n",
        "        for (j in 1:N_stations){\n",
        "            Sigma_eta[i,j]=(1/tau_eta)*exp(-phi_eta_esti*dist[i,j]);\n",
        "            Sigma_gamma[i,j]=(1/tau_gamma)*exp(-phi_gamma_esti*dist[i,j]);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    matrix[N_stations, N_stations] Sigma_delta;\n",
        "    for(i in 1:N_stations){\n",
        "      for(j in 1:N_stations){\n",
        "        real s = 0;\n",
        "        for(k in 1:N_covariates){\n",
        "          // Extract k-th column of A and multiply by its transpose\n",
        "          vector[N_covariates] column_k = A_raw[:, k];\n",
        "          real t_k = column_k'*column_k;\n",
        "          s += exp(-0.049*dist[i,j])*t_k;\n",
        "        }\n",
        "        Sigma_delta[i,j] = s;\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\n",
        "model {\n",
        "    vector[N_stations] zero = rep_vector(0,N_stations);\n",
        "    rho ~ normal(0,1);\n",
        "    xi ~ normal(0, s0);//check if a for loop is needed\n",
        "    beta ~ normal(0, s0);//same\n",
        "    tau_eta ~ gamma(a, b);\n",
        "    tau_gamma ~ gamma(a, b);\n",
        "    tau_epsilon ~ gamma(a, b);\n",
        "    mu ~ normal(0,s0);//same\n",
        "    // Priors for the elements of the lower triangular matrix\n",
        "    for (j in 1:N_covariates) {\n",
        "      for (i in 1:j) {\n",
        "        A_raw[i, j] ~ normal(0, 1);  // Prior for the elements\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Prior fo the phi_delta\n",
        "    /*for (k in 1:N_covariates){\n",
        "      phi_delta[k] ~ uniform(0.001,0.1);\n",
        "    }*/\n",
        "    // define the distribution of delta\n",
        "    for (i in 1:N_years){\n",
        "      for (j in 1:N_weeks){\n",
        "        for (p in 1:N_covariates){\n",
        "          delta[i][j][:, p] ~ multi_normal(zero, Sigma_gamma);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    for (i in 1:N_years){\n",
        "      for (j in 1:N_weeks){\n",
        "        to_vector(eta[i][j]) ~ multi_normal(zero,Sigma_eta);\n",
        "      }\n",
        "    }\n",
        "    for (i in 1:N_years){\n",
        "      to_vector(gamma[i]) ~ multi_normal(zero,Sigma_gamma);\n",
        "    }\n",
        "    for (i in 1:N_years) {\n",
        "      y[i][1] ~ normal(rep_vector(mu[i],N_stations)+to_vector(gamma[i]), 1/tau_epsilon);\n",
        "      for (j in 2:N_weeks){\n",
        "        y[i][j] ~ normal(rho*to_vector(y[i][j-1])+rep_vector(xi[i],N_stations)+delta[i][j]*to_vector(beta)+to_vector(eta[i][j]),1/tau_epsilon);//Check matrix product\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\n",
        "generated quantities {\n",
        "    vector[N_stations] zero = rep_vector(0,N_stations);\n",
        "    // Posterior predictive distribution\n",
        "    array[N_years_to_pred, N_stations_to_pred] real gamma_pred_sim; // represents values of gamma in a new location\n",
        "    array[N_years_to_pred,to_int(max(weeks_to_pred))] matrix[N_stations,N_covariates] delta_pred_sim_t; // represents the array of prediction of delta value in new time\n",
        "    array[N_years_to_pred,to_int(max(weeks_to_pred))] matrix[N_stations_to_pred,N_covariates] delta_pred_sim_s_t; // represents the array of prediction of delta value in new time and new location\n",
        "    array[N_years_to_pred,to_int(max(weeks_to_pred)),N_stations] real o_pred_sim_t; // represents the O value predicted for new time\n",
        "    array[N_years_to_pred,to_int(max(weeks_to_pred)),N_stations_to_pred] real o_pred_sim_s_t; // represents the O value predicted for a new location\n",
        "    array[N_years_to_pred,N_weeks_to_pred,N_stations_to_pred] real y_pred_sim;// represents the array of values for ozon level predicted for new time and new location\n",
        "\n",
        "    // Calculate the derived quantity Sigma_gamma_12\n",
        "    matrix[N_stations_to_pred, N_stations] Sigma_gamma_12;\n",
        "    for (i in 1:N_stations_to_pred){\n",
        "      for (j in 1:N_stations){\n",
        "        Sigma_gamma_12[i,j] = exp(-phi_gamma_esti * dist_pred[i][j]);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Claculate the derived quantity Sigma_eta_12\n",
        "    matrix[N_stations_to_pred, N_stations] Sigma_eta_12;\n",
        "    for (i in 1:N_stations_to_pred){\n",
        "      for (j in 1:N_stations){\n",
        "        Sigma_eta_12[i,j] = exp(-phi_eta_esti * dist_pred[i][j]);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Inverse of Sigma_gamma\n",
        "    matrix[N_stations, N_stations] Sigma_gamma_inv;\n",
        "    Sigma_gamma_inv = inverse(Sigma_gamma);\n",
        "\n",
        "    // Inverse of Sigma_eta\n",
        "    matrix[N_stations, N_stations] Sigma_eta_inv;\n",
        "    Sigma_eta_inv = inverse(Sigma_eta);\n",
        "\n",
        "    // Draw gamma_pred_sim from its posterior distribution\n",
        "    for (i in 1:N_years_to_pred){\n",
        "      for (j in 1:N_stations_to_pred){\n",
        "        gamma_pred_sim[i,j] = normal_rng(Sigma_gamma_12[i, ] * Sigma_gamma_inv * to_vector(gamma[i]), (1/tau_gamma) * (1 - Sigma_gamma_12[i, ] * Sigma_gamma_inv * Sigma_gamma_12[i, ]'));\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Calculate the derivded quantity Sigma_delta_12\n",
        "    matrix[N_stations_to_pred, N_stations] Sigma_delta_12;\n",
        "    for(i in 1:N_stations_to_pred){\n",
        "      for(j in 1:N_stations){\n",
        "        real s = 0;\n",
        "        for(k in 1:N_covariates){\n",
        "          // Extract k-th column of A and multiply by its transpose\n",
        "          vector[N_covariates] column_k = A_raw[:, k];\n",
        "          real t_k = column_k'*column_k;\n",
        "          s += exp(-0.049*dist_pred[i,j])*t_k;\n",
        "        }\n",
        "        Sigma_delta_12[i,j] = s;\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Calculate the derivded quantity Sigma_delta_11\n",
        "    matrix[N_stations_to_pred, N_stations_to_pred] Sigma_delta_11;\n",
        "    for(i in 1:N_stations_to_pred){\n",
        "      for(j in 1:N_stations_to_pred){\n",
        "        real s = 0;\n",
        "        for(k in 1:N_covariates){\n",
        "          // Extract k-th column of A and multiply by its transpose\n",
        "          vector[N_covariates] column_k = A_raw[:, k];\n",
        "          real t_k = column_k'*column_k;\n",
        "          s += exp(-0.049*dist_pred_to_pred[i,j])*t_k;\n",
        "        }\n",
        "        Sigma_delta_11[i,j] = s;\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Draw delta_pred_sim_t from its posterior predictive distribution\n",
        "    for (i in 1:N_years_to_pred){\n",
        "      for (j in 1:N_weeks_to_pred){\n",
        "        for (p in 1:N_covariates){\n",
        "          delta_pred_sim_t[i][j][:, p] = multi_normal_rng(zero, Sigma_delta);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    // Draw delta_pred_sim_s_t from its posterior predictive distribution\n",
        "    for (i in 1:N_years_to_pred){\n",
        "      for (j in 1:N_weeks_to_pred){\n",
        "        for (p in 1:N_covariates){\n",
        "          delta_pred_sim_s_t[i][j][:, p] = multi_normal_rng(Sigma_delta_12 * inverse(Sigma_delta) * delta_pred_sim_t[i][j][:, p], Sigma_delta_11 - Sigma_delta_12 * inverse(Sigma_delta) * transpose(Sigma_delta_12));\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Draw o_pred_sim_t from its posterior predictive distribution\n",
        "    for (i in 1:N_years_to_pred){\n",
        "      o_pred_sim_t[i][1] = to_array_1d(to_vector(gamma[i]) + mu[i]);\n",
        "      for (j in 2:to_int(max(weeks_to_pred))){\n",
        "        o_pred_sim_t[i][j] = to_array_1d(multi_normal_rng(xi[i] + rho *  to_vector(o_pred_sim_t[i][j - 1]) + delta_pred_sim_t[i][j] * to_vector(beta), (1/tau_eta) * Sigma_eta));\n",
        "      }\n",
        "    }\n",
        "    //Draw o_pred_sim_s_t from its posterior predictive distribution\n",
        "    for (i in 1:N_years_to_pred){\n",
        "      for (j in 1:N_stations_to_pred){\n",
        "        o_pred_sim_s_t[i][1][j] = gamma_pred_sim[i,j] + mu[i];\n",
        "        for (t in 2:to_int(max(weeks_to_pred))){\n",
        "          real G_delta = (1/tau_eta) * (1 - Sigma_eta_12[j, ] * Sigma_eta_inv * Sigma_eta_12[j, ]');\n",
        "          real arg_1 = xi[i] + rho * o_pred_sim_s_t[i][t-1][j] + delta_pred_sim_s_t[i][t][j] * to_vector(beta);\n",
        "          real arg_2 = Sigma_eta_12[j, ] * Sigma_eta_inv * (to_vector(o_pred_sim_t[i][j]) - xi[i] - rho *  to_vector(o_pred_sim_t[i][t - 1]) - delta_pred_sim_t[i][j] * to_vector(beta));\n",
        "          real zeta = arg_1 + arg_2;\n",
        "          o_pred_sim_s_t[i][t][j] = normal_rng(zeta, abs(G_delta));\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Draw the Ozon Level in the new location and new time from its posterior predictive distribution\n",
        "    for (i in 1:N_years_to_pred){\n",
        "      for (j in 1:N_weeks_to_pred){\n",
        "        for (k in 1:N_stations_to_pred){\n",
        "          y_pred_sim[i][j][k] = normal_rng(o_pred_sim_s_t[i][to_int(weeks_to_pred[j])][k], 1/tau_epsilon);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "# Write model to file\n",
        "stan_file = \"stan/spt_lm.stan\"\n",
        "with open(stan_file, \"w\") as f:\n",
        "    print(spt_lm, file=f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFOkYAluy34H"
      },
      "outputs": [],
      "source": [
        "# Compile model\n",
        "stan_file = \"stan/spt_lm.stan\"\n",
        "spt_lm_eff = CmdStanModel(stan_file=stan_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyviBgZuy55E"
      },
      "outputs": [],
      "source": [
        "spt_lm_no_delta = \\\n",
        "\"\"\"\n",
        "data {\n",
        "    int<lower=0> N_years;\n",
        "    int<lower=0> N_weeks;\n",
        "    int<lower=0> N_stations;\n",
        "    int<lower=0> N_covariates;\n",
        "\n",
        "    int<lower=0> N_years_to_pred;\n",
        "    int<lower=0> N_weeks_to_pred;\n",
        "    int<lower=0> N_stations_to_pred;\n",
        "\n",
        "    array[N_years,N_weeks,N_stations] real y;\n",
        "    array[N_years,N_weeks] matrix[N_stations,N_covariates] delta;\n",
        "    matrix[N_stations,N_stations] dist; //distance matrix\n",
        "\n",
        "    matrix[N_stations_to_pred, N_stations] dist_pred; // distance matrix which represents the distance from the stations to predict to every other station in the data set\n",
        "    matrix[N_stations_to_pred, N_stations_to_pred] dist_pred_to_pred; // distance matrix which represents the distance between the stations to predict\n",
        "    vector[N_weeks_to_pred] weeks_to_pred; // represents the time on which we want to perform prediction\n",
        "    array[N_years_to_pred,to_int(max(weeks_to_pred))] matrix[N_stations,N_covariates] delta_pred_sim_t; // represents the array of delta value in the weeks on which we want to do the prediction in the observed location\n",
        "    array[N_years_to_pred,to_int(max(weeks_to_pred))] matrix[N_stations_to_pred,N_covariates] delta_pred_sim_s_t; // represents the array of delta value in the weeks and sensors on which we want to do the prdiction\n",
        "\n",
        "    real<lower=0> a;\n",
        "    real<lower=0> b;\n",
        "    real<lower=0> s0;\n",
        "    real<lower=0> phi_gamma_esti;\n",
        "    real<lower=0> phi_eta_esti;\n",
        "}\n",
        "\n",
        "parameters {\n",
        "    real<lower=0,upper=1> rho;\n",
        "    array[N_years] real xi;\n",
        "    array[N_covariates] real beta;\n",
        "    array[N_years,N_weeks,N_stations] real eta;\n",
        "    array[N_years] real mu;\n",
        "    array[N_years,N_stations] real gamma;\n",
        "\n",
        "    real<lower=0> tau_eta;\n",
        "    real<lower=0> tau_gamma;\n",
        "    real<lower=0> tau_epsilon;\n",
        "}\n",
        "\n",
        "transformed parameters {\n",
        "    matrix[N_stations,N_stations] Sigma_eta;\n",
        "    matrix[N_stations,N_stations] Sigma_gamma;\n",
        "    for (i in 1:N_stations){\n",
        "        for (j in 1:N_stations){\n",
        "            Sigma_eta[i,j]=(1/tau_eta)*exp(-phi_eta_esti*dist[i,j]);\n",
        "            Sigma_gamma[i,j]=(1/tau_gamma)*exp(-phi_gamma_esti*dist[i,j]);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "model {\n",
        "    vector[N_stations] zero = rep_vector(0,N_stations);\n",
        "    rho ~ normal(0,1);\n",
        "    xi ~ normal(0, s0);//check if a for loop is needed\n",
        "    beta ~ normal(0, s0);//same\n",
        "    tau_eta ~ gamma(a, b);\n",
        "    tau_gamma ~ gamma(a, b);\n",
        "    tau_epsilon ~ gamma(a, b);\n",
        "    mu ~ normal(0,s0);//same\n",
        "\n",
        "    for (i in 1:N_years){\n",
        "      for (j in 1:N_weeks){\n",
        "        to_vector(eta[i][j]) ~ multi_normal(zero,Sigma_eta);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    for (i in 1:N_years){\n",
        "      to_vector(gamma[i]) ~ multi_normal(zero,Sigma_gamma);\n",
        "    }\n",
        "\n",
        "    for (i in 1:N_years) {\n",
        "      y[i][1] ~ normal(rep_vector(mu[i],N_stations)+to_vector(gamma[i]), 1/tau_epsilon);\n",
        "      for (j in 2:N_weeks){\n",
        "        y[i][j] ~ normal(rho*to_vector(y[i][j-1])+rep_vector(xi[i],N_stations)+delta[i][j]*to_vector(beta)+to_vector(eta[i][j]),1/tau_epsilon);//Check matrix product\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\n",
        "generated quantities {\n",
        "    vector[N_stations] zero = rep_vector(0,N_stations);\n",
        "    // Posterior predictive distribution\n",
        "    array[N_years_to_pred, N_stations_to_pred] real gamma_pred_sim; // represents values of gamma in a new location\n",
        "    array[N_years_to_pred,to_int(max(weeks_to_pred)),N_stations] real o_pred_sim_t; // represents the O value predicted for new time\n",
        "    array[N_years_to_pred,to_int(max(weeks_to_pred)),N_stations_to_pred] real o_pred_sim_s_t; // represents the O value predicted for a new location\n",
        "    array[N_years_to_pred,N_weeks_to_pred,N_stations_to_pred] real y_pred_sim;// represents the array of values for ozon level predicted for new time and new location\n",
        "\n",
        "    // Calculate the derived quantity Sigma_gamma_12\n",
        "    matrix[N_stations_to_pred, N_stations] Sigma_gamma_12;\n",
        "    for (i in 1:N_stations_to_pred){\n",
        "      for (j in 1:N_stations){\n",
        "        Sigma_gamma_12[i,j] = exp(-phi_gamma_esti * dist_pred[i][j]);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Claculate the derived quantity Sigma_eta_12\n",
        "    matrix[N_stations_to_pred, N_stations] Sigma_eta_12;\n",
        "    for (i in 1:N_stations_to_pred){\n",
        "      for (j in 1:N_stations){\n",
        "        Sigma_eta_12[i,j] = exp(-phi_eta_esti * dist_pred[i][j]);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Inverse of Sigma_gamma\n",
        "    matrix[N_stations, N_stations] Sigma_gamma_inv;\n",
        "    Sigma_gamma_inv = inverse(Sigma_gamma);\n",
        "\n",
        "    // Inverse of Sigma_eta\n",
        "    matrix[N_stations, N_stations] Sigma_eta_inv;\n",
        "    Sigma_eta_inv = inverse(Sigma_eta);\n",
        "\n",
        "    // Draw gamma_pred_sim from its posterior distribution\n",
        "    for (i in 1:N_years_to_pred){\n",
        "      for (j in 1:N_stations_to_pred){\n",
        "        gamma_pred_sim[i,j] = normal_rng(Sigma_gamma_12[i, ] * Sigma_gamma_inv * to_vector(gamma[i]), (1/tau_gamma) * (1 - Sigma_gamma_12[i, ] * Sigma_gamma_inv * Sigma_gamma_12[i, ]'));\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Draw o_pred_sim_t from its posterior predictive distribution\n",
        "    for (i in 1:N_years_to_pred){\n",
        "      o_pred_sim_t[i][1] = to_array_1d(to_vector(gamma[i]) + mu[i]);\n",
        "      for (j in 2:to_int(max(weeks_to_pred))){\n",
        "        o_pred_sim_t[i][j] = to_array_1d(multi_normal_rng(xi[i] + rho *  to_vector(o_pred_sim_t[i][j - 1]) + delta_pred_sim_t[i][j] * to_vector(beta), (1/tau_eta) * Sigma_eta));\n",
        "      }\n",
        "    }\n",
        "    //Draw o_pred_sim_s_t from its posterior predictive distribution\n",
        "    for (i in 1:N_years_to_pred){\n",
        "      for (j in 1:N_stations_to_pred){\n",
        "        o_pred_sim_s_t[i][1][j] = gamma_pred_sim[i,j] + mu[i];\n",
        "        for (t in 2:to_int(max(weeks_to_pred))){\n",
        "          real G_delta = (1/tau_eta) * (1 - Sigma_eta_12[j, ] * Sigma_eta_inv * Sigma_eta_12[j, ]');\n",
        "          real arg_1 = xi[i] + rho * o_pred_sim_s_t[i][t-1][j] + delta_pred_sim_s_t[i][t][j] * to_vector(beta);\n",
        "          real arg_2 = Sigma_eta_12[j, ] * Sigma_eta_inv * (to_vector(o_pred_sim_t[i][j]) - xi[i] - rho *  to_vector(o_pred_sim_t[i][t - 1]) - delta_pred_sim_t[i][j] * to_vector(beta));\n",
        "          real zeta = arg_1 + arg_2;\n",
        "          o_pred_sim_s_t[i][t][j] = normal_rng(zeta, abs(G_delta));\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Draw the Ozon Level in the new location and new time from its posterior predictive distribution\n",
        "    for (i in 1:N_years_to_pred){\n",
        "      for (j in 1:N_weeks_to_pred){\n",
        "        for (k in 1:N_stations_to_pred){\n",
        "          y_pred_sim[i][j][k] = normal_rng(o_pred_sim_s_t[i][to_int(weeks_to_pred[j])][k], 1/tau_epsilon);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "# Write model to file\n",
        "stan_file = \"stan/spt_lm_no_delta.stan\"\n",
        "with open(stan_file, \"w\") as f:\n",
        "    print(spt_lm_no_delta, file=f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EQ9IqrPy8Is"
      },
      "outputs": [],
      "source": [
        "# Compile the model without delta distribution\n",
        "stan_file_no_delta = \"stan/spt_lm_no_delta.stan\"\n",
        "spt_lm_no_delta_eff = CmdStanModel(stan_file=stan_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fVFb5cP0KLU"
      },
      "source": [
        "### Prepare Model to be Ran"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgpfAmZl0VPv"
      },
      "source": [
        "Dealing with Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezIMgkQK0L0R"
      },
      "outputs": [],
      "source": [
        "indici_righe_manca_valore = df_final[df_final[\"Valore\"].isnull()].index\n",
        "\n",
        "for indice in indici_righe_manca_valore:\n",
        "    # Check if the indices before and after are not null\n",
        "    if (indice - 1) in df_final.index and (indice + 1) in df_final.index:\n",
        "        # Check if they have the same idSensore\n",
        "        idSensore_corrente = df_final.at[indice, \"idSensore\"]\n",
        "        idSensore_precedente = df_final.at[indice - 1, \"idSensore\"]\n",
        "        idSensore_successivo = df_final.at[indice + 1, \"idSensore\"]\n",
        "\n",
        "        if idSensore_corrente == idSensore_precedente == idSensore_successivo:\n",
        "            # Compute the mean of the two \"Valore\" and fill the missing value\n",
        "            valore_precedente = df_final.at[indice - 1, \"Valore\"]\n",
        "            valore_successivo = df_final.at[indice + 1, \"Valore\"]\n",
        "            media_valori = (valore_precedente + valore_successivo) / 2\n",
        "\n",
        "            # Fill the missing value with the mean\n",
        "            df_final.at[indice, \"Valore\"] = media_valori\n",
        "\n",
        "#indici_righe_manca_valore = df_final[df_final[\"Valore\"].isnull()].index\n",
        "#print(indici_righe_manca_valore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v21HQnrz0s0J"
      },
      "outputs": [],
      "source": [
        "spec=np.linspace(df_final.at[1769,'Valore'],df_final.at[1772,'Valore'], 4)\n",
        "df_final.at[1770,'Valore']=spec[1]\n",
        "df_final.at[1771,'Valore']=spec[2]\n",
        "spec=np.linspace(df_final.at[1991,'Valore'],df_final.at[1994,'Valore'], 4)\n",
        "df_final.at[1992,'Valore']=spec[1]\n",
        "df_final.at[1993,'Valore']=spec[2]\n",
        "spec=np.linspace(df_final.at[2833,'Valore'],df_final.at[2836,'Valore'], 4)\n",
        "df_final.at[2834,'Valore']=spec[1]\n",
        "df_final.at[2835,'Valore']=spec[2]\n",
        "spec=np.linspace(df_final.at[4183,'Valore'],df_final.at[4186,'Valore'], 4)\n",
        "df_final.at[4184,'Valore']=spec[1]\n",
        "df_final.at[4185,'Valore']=spec[2]\n",
        "spec=np.linspace(df_final.at[7961,'Valore'],df_final.at[7964,'Valore'], 4)\n",
        "df_final.at[7962,'Valore']=spec[1]\n",
        "df_final.at[7963,'Valore']=spec[2]\n",
        "spec=np.linspace(df_final.at[10531,'Valore'],df_final.at[10534,'Valore'], 4)\n",
        "df_final.at[10532,'Valore']=spec[1]\n",
        "df_final.at[10533,'Valore']=spec[2]\n",
        "spec=np.linspace(df_final.at[11006,'Valore'],df_final.at[11009,'Valore'], 4)\n",
        "df_final.at[11007,'Valore']=spec[1]\n",
        "df_final.at[11008,'Valore']=spec[2]\n",
        "spec=np.linspace(df_final.at[13248,'Valore'],df_final.at[13251,'Valore'], 4)\n",
        "df_final.at[13249,'Valore']=spec[1]\n",
        "df_final.at[13250,'Valore']=spec[2]\n",
        "spec=np.linspace(df_final.at[13422,'Valore'],df_final.at[13425,'Valore'], 4)\n",
        "df_final.at[13423,'Valore']=spec[1]\n",
        "df_final.at[13424,'Valore']=spec[2]\n",
        "spec=np.linspace(df_final.at[13971,'Valore'],df_final.at[13974,'Valore'], 4)\n",
        "df_final.at[13972,'Valore']=spec[1]\n",
        "df_final.at[13973,'Valore']=spec[2]\n",
        "df_final.at[15028,'Valore']=df_final.at[15030,'Valore']\n",
        "df_final.at[15029,'Valore']=df_final.at[15030,'Valore']\n",
        "df_final.at[17954,'Valore']=df_final.at[17956,'Valore']\n",
        "df_final.at[17955,'Valore']=df_final.at[17956,'Valore']\n",
        "spec=np.linspace(df_final.at[2915,'Valore'],df_final.at[2919,'Valore'], 5)\n",
        "df_final.at[2916,'Valore']=spec[1]\n",
        "df_final.at[2917,'Valore']=spec[2]\n",
        "df_final.at[2918,'Valore']=spec[3]\n",
        "spec=np.linspace(df_final.at[8033,'Valore'],df_final.at[8037,'Valore'], 5)\n",
        "df_final.at[8034,'Valore']=spec[1]\n",
        "df_final.at[8035,'Valore']=spec[2]\n",
        "df_final.at[8036,'Valore']=spec[3]\n",
        "spec=np.linspace(df_final.at[13421,'Valore'],df_final.at[13425,'Valore'], 5)\n",
        "df_final.at[13422,'Valore']=spec[1]\n",
        "df_final.at[13423,'Valore']=spec[2]\n",
        "df_final.at[13424,'Valore']=spec[3]\n",
        "spec=np.linspace(df_final.at[17237,'Valore'],df_final.at[17241,'Valore'], 5)\n",
        "df_final.at[17238,'Valore']=spec[1]\n",
        "df_final.at[17239,'Valore']=spec[2]\n",
        "df_final.at[17240,'Valore']=spec[3]\n",
        "df_final.at[18708,'Valore']=df_final.at[18711,'Valore']\n",
        "df_final.at[18709,'Valore']=df_final.at[18711,'Valore']\n",
        "df_final.at[18710,'Valore']=df_final.at[18711,'Valore']\n",
        "df_final.at[19225,'Valore']=df_final.at[19228,'Valore']\n",
        "df_final.at[19226,'Valore']=df_final.at[19228,'Valore']\n",
        "df_final.at[19227,'Valore']=df_final.at[19228,'Valore']\n",
        "spec=np.linspace(df_final.at[19692,'Valore'],df_final.at[19696,'Valore'], 5)\n",
        "df_final.at[19693,'Valore']=spec[1]\n",
        "df_final.at[19694,'Valore']=spec[2]\n",
        "df_final.at[19695,'Valore']=spec[3]\n",
        "spec=np.linspace(df_final.at[19755,'Valore'],df_final.at[19759,'Valore'], 5)\n",
        "df_final.at[19756,'Valore']=spec[1]\n",
        "df_final.at[19757,'Valore']=spec[2]\n",
        "df_final.at[19758,'Valore']=spec[3]\n",
        "spec=np.linspace(df_final.at[18847,'Valore'],df_final.at[18852,'Valore'], 6)\n",
        "df_final.at[18848,'Valore']=spec[1]\n",
        "df_final.at[18849,'Valore']=spec[2]\n",
        "df_final.at[18850,'Valore']=spec[3]\n",
        "df_final.at[18851,'Valore']=spec[4]\n",
        "spec=np.linspace(df_final.at[17790,'Valore'],df_final.at[17799,'Valore'], 10)\n",
        "df_final.at[17791,'Valore']=spec[1]\n",
        "df_final.at[17792,'Valore']=spec[2]\n",
        "df_final.at[17793,'Valore']=spec[3]\n",
        "df_final.at[17794,'Valore']=spec[4]\n",
        "df_final.at[17795,'Valore']=spec[5]\n",
        "df_final.at[17796,'Valore']=spec[6]\n",
        "df_final.at[17797,'Valore']=spec[7]\n",
        "df_final.at[17798,'Valore']=spec[8]\n",
        "df_final.at[12103,'Valore']=df_final.at[12102,'Valore']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fWUdYRP0v1n"
      },
      "outputs": [],
      "source": [
        "df_final['Valore_log'] = np.log(df_final['Valore'].replace(0, np.nan)) # Apply the log transformation as per the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coCUCa4W1Ix6"
      },
      "source": [
        "Validation Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmxyjvlV03PB"
      },
      "outputs": [],
      "source": [
        "validation_ids = [10463.0, 10584.0, 12020.0, 17288.0, 17295.0, 17297.0, 20041.0, 20154.0, 30165.0 ]\n",
        "\n",
        "validation_set = df_copy[df_copy['idSensore'].isin(validation_ids)]\n",
        "\n",
        "df_final = df_copy.drop(validation_set.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfcJemZz1RPE"
      },
      "source": [
        "### Running Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEOPGI871QBu"
      },
      "outputs": [],
      "source": [
        "standardized_data = standardize_columns()\n",
        "\n",
        "start_date = '2010-01-01'\n",
        "end_date = '2010-12-31'\n",
        "\n",
        "covariate_columns = ['precipitation_sum (mm)', 'rain_sum (mm)', 'snowfall_sum (cm)',\n",
        "                    'wind_speed_10m_max (km/h)', 'temperature_2m_mean (°C)',\n",
        "                    'apparent_temperature_max (°C)', 'sunshine_duration (s)',\n",
        "                    'wind_gusts_10m_max (km/h)', 'wind_direction_10m_dominant (°)',\n",
        "                    'shortwave_radiation_sum (MJ/m²)', 'relative_humidity_2m (%)']\n",
        "\n",
        "columns_to_include = ['idSensore', 'Valore_log', 'Data', 'location','location_id'] + covariate_columns\n",
        "\n",
        "filtered_data_for_stan = filter_dataframe_by_date_and_columns(start_date, end_date, columns_to_include, standardized_data)\n",
        "processed_data_for_stan = process_and_add_deltas(filtered_data_for_stan, columns_to_increment = covariate_columns)\n",
        "\n",
        "valore_array = fill_valore_array(processed_data_for_stan)\n",
        "delta_array = fill_delta_array(processed_data_for_stan, n_covariates = len(covariate_columns))\n",
        "distance_matrix = calculate_distance_matrix(processed_data_for_stan)\n",
        "\n",
        "n_years = valore_array.shape[0]\n",
        "n_weeks = valore_array.shape[1]\n",
        "n_sensors = valore_array.shape[2]\n",
        "n_covariates = len(covariate_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uICAf4Kl1U62"
      },
      "outputs": [],
      "source": [
        "# For validation set\n",
        "standardized_validation = standardize_columns(validation_set)\n",
        "filtered_data_validation = filter_dataframe_by_date_and_columns(start_date, end_date, columns_to_include, validation_set)\n",
        "processed_data_validation = process_and_add_deltas(filtered_data_validation, columns_to_increment = covariate_columns)\n",
        "\n",
        "valore_array_validation = fill_valore_array(processed_data_validation)\n",
        "delta_pred_sim_s_t = fill_delta_array(processed_data_validation, n_covariates = len(covariate_columns))\n",
        "distance_matrix_validation = calculate_distance_matrix(processed_data_validation)\n",
        "distance_matrix_pred = calculate_distance_matrix_pred(processed_data_for_stan, processed_data_validation)\n",
        "\n",
        "n_years_pred = valore_array_validation.shape[0]\n",
        "n_weeks_pred = valore_array_validation.shape[1]\n",
        "n_sensors_pred = valore_array_validation.shape[2]\n",
        "weeks_to_predict = [i for i in range(1,min(n_weeks_pred + 1, 54))]\n",
        "\n",
        "delta_pred_sim_t = delta_array[:,:np.max(weeks_to_predict),:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5oB_yLc1aOQ"
      },
      "outputs": [],
      "source": [
        "spt_lm_data = {\n",
        "    \"N_years\": n_years,\n",
        "    \"N_weeks\": n_weeks,\n",
        "    \"N_stations\": n_sensors,\n",
        "    \"N_covariates\": n_covariates,\n",
        "\n",
        "    \"y\": valore_array,\n",
        "    \"delta\": delta_array,\n",
        "    \"dist\": distance_matrix,\n",
        "\n",
        "    \"N_years_to_pred\": n_years_pred,\n",
        "    \"N_weeks_to_pred\": n_weeks_pred,\n",
        "    \"N_stations_to_pred\": n_sensors_pred,\n",
        "\n",
        "    \"dist_pred\": distance_matrix_pred,\n",
        "    \"dist_pred_to_pred\": distance_matrix_validation,\n",
        "    \"weeks_to_pred\": weeks_to_predict,\n",
        "\n",
        "    \"a\": 2,\n",
        "    \"b\": 1,\n",
        "    \"s0\": 1,\n",
        "    \"phi_gamma_esti\": 0.05,\n",
        "    \"phi_eta_esti\": 0.005\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOmYl8U-1ej-"
      },
      "outputs": [],
      "source": [
        "# Run the model\n",
        "spt_lm_fit = spt_lm_eff.sample(\n",
        "    data=spt_lm_data, chains=4, parallel_chains=4,\n",
        "    iter_warmup=1000, iter_sampling=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAAVbW1Z1h1c"
      },
      "outputs": [],
      "source": [
        "spt_lm_eff_data = az.from_cmdstanpy(spt_lm_fit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FahW5u-O1ii4"
      },
      "outputs": [],
      "source": [
        "az.plot_trace(spt_lm_eff_data, var_names=[\n",
        "    \"rho\"], compact=False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YZ3e1Eg1oHx"
      },
      "outputs": [],
      "source": [
        "output_data_summary = spt_lm_fit.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuZCw_q_1qvG"
      },
      "outputs": [],
      "source": [
        "output_data_summary.loc[\"Sigma_eta[1,1]\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcZxMPiP1tSL"
      },
      "outputs": [],
      "source": [
        "new_quantities = spt_lm_eff.generate_quantities(data=spt_lm_data, previous_fit=spt_lm_fit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEzJe-Dn1vr2"
      },
      "outputs": [],
      "source": [
        "spt_lm_no_delta_data = {\n",
        "    \"N_years\": n_years,\n",
        "    \"N_weeks\": n_weeks,\n",
        "    \"N_stations\": n_sensors,\n",
        "    \"N_covariates\": n_covariates,\n",
        "\n",
        "    \"y\": valore_array,\n",
        "    \"delta\": delta_array,\n",
        "    \"dist\": distance_matrix,\n",
        "\n",
        "    \"N_years_to_pred\": n_years_pred,\n",
        "    \"N_weeks_to_pred\": n_weeks_pred,\n",
        "    \"N_stations_to_pred\": n_sensors_pred,\n",
        "    \"delta_pred_sim_s_t\": delta_pred_sim_s_t,\n",
        "    \"delta_pred_sim_t\": delta_pred_sim_t,\n",
        "\n",
        "    \"dist_pred\": distance_matrix_pred,\n",
        "    \"dist_pred_to_pred\": distance_matrix_validation,\n",
        "    \"weeks_to_pred\": weeks_to_predict,\n",
        "\n",
        "    \"a\": 2,\n",
        "    \"b\": 1,\n",
        "    \"s0\": 1,\n",
        "    \"phi_gamma_esti\": 0.05,\n",
        "    \"phi_eta_esti\": 0.005\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un-BHObX1y1u"
      },
      "outputs": [],
      "source": [
        "# Run the model withour the delta distribution\n",
        "spt_lm_no_delta_fit = spt_lm_no_delta_eff.sample(\n",
        "    data=spt_lm_no_delta_data, chains=4, parallel_chains=4,\n",
        "    iter_warmup=1000, iter_sampling=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0Zk6MK9117V"
      },
      "outputs": [],
      "source": [
        "spt_lm_no_delta_eff_data = az.from_cmdstanpy(spt_lm_no_delta_fit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U1fg7uM13vH"
      },
      "outputs": [],
      "source": [
        "az.plot_trace(spt_lm_no_delta_eff_data, var_names=[\n",
        "    \"rho\"], compact=False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQAHGYEo15vq"
      },
      "outputs": [],
      "source": [
        "output_no_delta_data_summary = spt_lm_no_delta_fit.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-M3cmz418KR"
      },
      "outputs": [],
      "source": [
        "# FIRST METHOD\n",
        "\n",
        "# Set the 'Unnamed: 0' column as the index\n",
        "#output_no_delta_data_summary.set_index('Unnamed: 0', inplace=True)\n",
        "\n",
        "# Select the covariates you're interested in\n",
        "#selected_covariates = output_no_delta_data_summary.loc[['xi[1]', 'beta[1]', 'beta[2]', 'beta[3]', 'beta[4]', 'beta[5]', 'beta[6]', 'beta[7]', 'beta[8]', 'beta[9]', 'beta[10]', 'beta[11]']]\n",
        "selected_covariates = output_no_delta_data_summary.loc[['xi[1]', 'beta[1]', 'beta[2]', 'beta[3]']]\n",
        "\n",
        "# Calculate confidence intervals for the selected covariates\n",
        "confidence_intervals = {}\n",
        "for covariate in selected_covariates.index:\n",
        "    lower_bound = selected_covariates.loc[covariate, '5%']\n",
        "    upper_bound = selected_covariates.loc[covariate, '95%']\n",
        "    confidence_intervals[covariate] = (lower_bound, upper_bound)\n",
        "\n",
        "# Print confidence intervals\n",
        "for covariate, interval in confidence_intervals.items():\n",
        "    print(f\"{covariate}: {interval}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HISbEyj51-hq"
      },
      "outputs": [],
      "source": [
        "# SECOND METHOD\n",
        "chains_finish = az.from_cmdstanpy(spt_lm_no_delta_fit)\n",
        "# Assuming you have a single chain named chain_finish\n",
        "chain = chains_finish\n",
        "names = [\"Single Chain\"]  # Rename if desired\n",
        "\n",
        "# Assuming you have a variable named cov_names that contains the names of the covariates\n",
        "# Replace this with your actual list of covariate names\n",
        "#cov_names = [\"beta1\", \"beta2\", \"beta3\", \"beta4\", \"beta5\", \"beta6\", \"beta7\", \"beta8\", \"beta9\", \"beta10\", \"beta11\",]\n",
        "cov_names = ['beta[1]', 'beta[2]', 'beta[3]']\n",
        "\n",
        "beta_chain = np.vstack(chain.posterior.beta)\n",
        "hdi_beta = az.hdi(beta_chain, hdi_prob=0.95)\n",
        "\n",
        "keep = np.array([~(hdi_beta[i, 0] <= 0.0 <= hdi_beta[i, 1]) for i in range(hdi_beta.shape[0])])\n",
        "\n",
        "# Print out the names of covariates that do not contain zero within their HDI\n",
        "print(\"{0}:\\n{1}\\n\".format(names[0], [cov_names[i] for i, k in enumerate(keep) if k]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFXZfToN2A5d"
      },
      "outputs": [],
      "source": [
        "# GENERATED QUANTITIES\n",
        "new_no_delta_quantities = spt_lm_no_delta_eff.generate_quantities(data=spt_lm_no_delta_data, previous_fit=spt_lm_no_delta_fit)\n",
        "\n",
        "new_no_delta_quantities.draws().shape\n",
        "\n",
        "print(new_no_delta_quantities.draws().shape, new_no_delta_quantities.column_names)\n",
        "for i in range(3):\n",
        "    print (new_no_delta_quantities.draws()[i,:])\n",
        "\n",
        "sample_plus = new_no_delta_quantities.draws_pd(inc_sample=True)\n",
        "print(type(sample_plus),sample_plus.shape)\n",
        "names = list(sample_plus.columns.values[7:18])\n",
        "sample_plus.iloc[0:3, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSh2AM2P2DXc"
      },
      "outputs": [],
      "source": [
        "# PLOT\n",
        "\n",
        "# Assuming your validation set is named 'validation_set'\n",
        "# and it contains the real values for the ozone levels\n",
        "# For example, if your validation set has a column named 'ozone_level':\n",
        "real_values = processed_data_validation['Valore_log']\n",
        "\n",
        "# Extract the column names that start with 'y_pred_sim'\n",
        "y_pred_columns = [col for col in sample_plus.columns if col.startswith('y_pred_sim')]\n",
        "\n",
        "# Select only the columns corresponding to 'y_pred_sim[i,j,k]'\n",
        "predicted_values = sample_plus[y_pred_columns].values.mean(axis = 0)\n",
        "\n",
        "# Plot the predicted values against the real values\n",
        "plt.scatter(valore_array_validation, predicted_values, alpha=0.5)\n",
        "plt.xlabel('Real Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Predicted vs Real Values')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
