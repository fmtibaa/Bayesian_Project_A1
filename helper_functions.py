# -*- coding: utf-8 -*-
"""Helper_functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NJGntvkATVTFzbTPy2nnRgh4eMAZZC_b
"""

from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

def standardize_columns(input_df=None):
    """
    Standardizes specified columns in a DataFrame using StandardScaler.

    Parameters:
    - input_df: pd.DataFrame, optional
        The input DataFrame. Default is df_final.

    Returns:
    - pd.DataFrame
        The DataFrame with specified columns standardized.
    """
    # If input_df is not provided, use df_final
    if input_df is None:
        input_df = df_final.copy()

    # Specify columns to standardize
    columns_to_standardize = ['precipitation_sum (mm)', 'rain_sum (mm)', 'snowfall_sum (cm)',
                               'wind_speed_10m_max (km/h)', 'temperature_2m_mean (°C)',
                               'apparent_temperature_max (°C)', 'sunshine_duration (s)',
                               'wind_gusts_10m_max (km/h)', 'wind_direction_10m_dominant (°)',
                               'shortwave_radiation_sum (MJ/m²)', 'relative_humidity_2m (%)']

    # Create a copy of the input DataFrame to avoid modifying the original
    standardized_df = input_df.copy()

    # Standardize specified columns
    scaler = StandardScaler()
    standardized_df[columns_to_standardize] = scaler.fit_transform(standardized_df[columns_to_standardize])

    return standardized_df

def filter_dataframe_by_date_and_columns(start_date, end_date, columns_to_include, input_df):
    """
    Filters a DataFrame based on a date range and includes specified columns.

    Parameters:
    - start_date: str
        The start date in 'YYYY-MM-DD' format.
    - end_date: str
        The end date in 'YYYY-MM-DD' format.
    - columns_to_include: list
        A list of column names to include in the final result.
    - input_df: pd.DataFrame, optional
        The input DataFrame to be filtered. Default is None, which uses df_final.

    Returns:
    - pd.DataFrame
        The filtered DataFrame with specified columns.
    """
    # If input_df is not provided, use df_final
    if input_df is None:
        input_df = df_final.copy()

    # Ensure 'Data' column is in datetime format
    input_df['Data'] = pd.to_datetime(input_df['Data'])

    # Filter rows between two dates
    filtered_df = input_df[(input_df['Data'] >= start_date) & (input_df['Data'] <= end_date)]

    # Select specified columns
    filtered_df = filtered_df[columns_to_include]

    return filtered_df

def process_and_add_deltas(df, date_column='Data', sensor_id_column='idSensore', columns_to_increment=None):
    """
    Sorts the DataFrame by sensor ID and date, adds weekly increments for specified columns,
    and replaces NaN values with zeros.

    Parameters:
    - df: pd.DataFrame
        The input DataFrame to be processed.
    - date_column: str, optional
        The name of the column containing date information. Default is 'Data'.
    - sensor_id_column: str, optional
        The name of the column containing sensor IDs. Default is 'idSensore'.
    - columns_to_increment: list, optional
        A list of column names for which weekly increments should be calculated.

    Returns:
    - pd.DataFrame
        The processed DataFrame with added weekly increments and NaN values replaced by zeros.
    """
    # Sort the DataFrame by sensor ID and date
    df.sort_values(by=[sensor_id_column, date_column], inplace=True)

    # Default columns to increment if not provided
    if columns_to_increment is None:
        columns_to_increment = ['temperature_2m_mean (°C)']

    # Creating for each weather variable, a column that has the weekly increments wrt week before
    for i, column in enumerate(columns_to_increment, start=1):
        delta_column = f'delta_{i}'
        df[delta_column] = df.groupby(sensor_id_column)[column].diff()

    # For the first row of a sensor's measurement, set the increment value to the same value of the measurment
    df.loc[df[sensor_id_column].diff() != 0, [f'delta_{i}' for i in range(1, len(columns_to_increment) + 1)]] = df.loc[df[sensor_id_column].diff() != 0, columns_to_increment].values

    return df

def fill_valore_array(data_df):
    """
    Fills a 3D NumPy array based on DataFrame values.

    Parameters:
    - data_df: pd.DataFrame
        The DataFrame containing the values to fill the array.
    - valore_array_shape: tuple, optional
        The shape of the NumPy array to be filled. Default is (1, None, None).

    Returns:
    - np.ndarray
        The filled 3D NumPy array.
    """
    years = data_df['Data'].dt.year.unique()
    sensors = data_df['idSensore'].unique()

    if (len(years) == 1):
      # Extract unique sensor IDs and weeks
      weeks = data_df['Data'].unique()

      valore_array_shape = (1, len(weeks), len(sensors))

      # Create an array of zeros based on the specified shape
      valore_array = np.zeros(valore_array_shape)

      # Fill the array based on DataFrame values
      for index, row in data_df.iterrows():
          year_index = 0  # Assuming year index is always 0
          week_index = np.where(weeks == row['Data'])[0][0]
          sensor_index = np.where(sensors == row['idSensore'])[0][0]

          valore_array[year_index, week_index, sensor_index] = row['Valore_log']

      return valore_array
    elif (len(years) > 1):
      # Create a dictionary to store DataFrames for each year
      yearly_data = {year: data_df[data_df['Data'].dt.year == year] for year in years}
      year_index = 0
      valore_array_shape = (len(years), 53, len(sensors))
      # Create an array of zeros based on the specified shape
      valore_array = np.zeros(valore_array_shape)
      for y in years:
        subset_df = yearly_data[y]
        weeks = subset_df['Data'].unique()

        # Fill the array based on DataFrame values
        for index, row in subset_df.iterrows():
            week_index = np.where(weeks == row['Data'])[0][0]
            sensor_index = np.where(sensors == row['idSensore'])[0][0]

            valore_array[year_index, week_index, sensor_index] = row['Valore_log']

        year_index += 1

      return valore_array

def fill_delta_array(processed_df, n_covariates=4):
    """
    Creates a 4D NumPy array (delta_array) based on processed DataFrame values.

    Parameters:
    - processed_df: pd.DataFrame
        The processed DataFrame containing delta values.
    - n_covariates: int, optional
        The number of covariates. Default is 4.

    Returns:
    - np.ndarray
        The 4D NumPy array containing delta values.
    """
    # Extract unique sensor IDs and weeks
    years = processed_df['Data'].dt.year.unique()
    sensors = processed_df['idSensore'].unique()

    if (len(years) == 1):
      weeks = processed_df['Data'].unique()

      delta_array = np.zeros((1, len(weeks), len(sensors), n_covariates))

      for index, row in processed_df.iterrows():
          year_index = 0
          week_index = np.where(weeks == row['Data'])[0][0]
          sensor_index = np.where(sensors == row['idSensore'])[0][0]

          # Iterate through all columns starting with 'delta_'
          for col_name in processed_df.columns:
              if col_name.startswith('delta_'):
                  # Extract the covariate index from the column name
                  covariate_index = int(col_name.split('_')[1]) - 1  # Subtract 1 to convert from 1-based to 0-based index

                  # Assign values to the delta_array
                  delta_array[year_index][week_index][sensor_index][covariate_index] = row[col_name]

      return delta_array

    elif (len(years) > 1):
      # Create a dictionary to store DataFrames for each year
      yearly_data = {year: processed_df[processed_df['Data'].dt.year == year] for year in years}
      year_index = 0
      delta_array = np.zeros((len(years), 53, len(sensors), n_covariates))

      for y in years:
        subset_df = yearly_data[y]
        weeks = subset_df['Data'].unique()

        for index, row in subset_df.iterrows():
            week_index = np.where(weeks == row['Data'])[0][0]
            sensor_index = np.where(sensors == row['idSensore'])[0][0]

            # Iterate through all columns starting with 'delta_'
            for col_name in subset_df.columns:
                if col_name.startswith('delta_'):
                    # Extract the covariate index from the column name
                    covariate_index = int(col_name.split('_')[1]) - 1  # Subtract 1 to convert from 1-based to 0-based index

                    # Assign values to the delta_array
                    delta_array[year_index][week_index][sensor_index][covariate_index] = row[col_name]
        year_index += 1

      return delta_array

def calculate_distance_matrix(data_df, id_column='location_id', location_column='location'):
    """
    Calculates the Euclidean distance matrix based on location data in a DataFrame.

    Parameters:
    - data_df: pd.DataFrame
        The DataFrame containing location data.
    - id_column: str, optional
        The name of the column containing IDs. Default is 'location_id'.
    - location_column: str, optional
        The name of the column containing location information. Default is 'location'.

    Returns:
    - pd.DataFrame
        The Euclidean distance matrix.
    """
    # Drop duplicates to get unique ID-value pairs
    unique_df_location = data_df[[id_column, location_column]].drop_duplicates()

    # Function to parse string and extract latitude, longitude as tuple
    def parse_point(point_str):
        lat, lon = map(float, point_str.strip('()').split(','))
        return lat, lon

    # Function to calculate Euclidean distance between two points
    def euclidean_distance(point1, point2):
        return np.linalg.norm(np.array(point1) - np.array(point2))

    # Parse points and compute distance matrix
    unique_df_location[location_column] = unique_df_location[location_column].apply(parse_point)
    points = unique_df_location[location_column].tolist()

    distance_matrix = pd.DataFrame(
        [[euclidean_distance(p1, p2) for p2 in points] for p1 in points],
        columns=unique_df_location[id_column], index=unique_df_location[id_column]
    )

    return distance_matrix

def calculate_distance_matrix_pred(data_df, data_df_to_pred, id_column='location_id', location_column='location'):
    """
    Calculates the Euclidean distance matrix based on location data in a DataFrame.

    Parameters:
    - data_df: pd.DataFrame
        The DataFrame containing location data.
    - data_df_to_pred: pd.DataFrame
        The DataFrame containing location data on which we want to do prediction
    - id_column: str, optional
        The name of the column containing IDs. Default is 'location_id'.
    - location_column: str, optional
        The name of the column containing location information. Default is 'location'.

    Returns:
    - pd.DataFrame
        The Euclidean distance matrix.
    """
    # Drop duplicates to get unique ID-value pairs
    unique_df_location = data_df[[id_column, location_column]].drop_duplicates()
    unique_df_to_pred_location = data_df_to_pred[[id_column, location_column]].drop_duplicates()

    # Function to parse string and extract latitude, longitude as tuple
    def parse_point(point_str):
        lat, lon = map(float, point_str.strip('()').split(','))
        return lat, lon

    # Function to calculate Euclidean distance between two points
    def euclidean_distance(point1, point2):
        return np.linalg.norm(np.array(point1) - np.array(point2))

    # Parse points and compute distance matrix
    unique_df_location[location_column] = unique_df_location[location_column].apply(parse_point)
    points = unique_df_location[location_column].tolist()
    unique_df_to_pred_location[location_column] = unique_df_to_pred_location[location_column].apply(parse_point)
    points_to_pred = unique_df_to_pred_location[location_column].tolist()

    distance_matrix = pd.DataFrame(
        [[euclidean_distance(p1, p2) for p2 in points] for p1 in points_to_pred],
        columns=unique_df_location[id_column], index=unique_df_to_pred_location[id_column]
    )

    return distance_matrix

def save_stan_fit_and_summary_with_data(stan_fit, summary_df, file_name_prefix="stan_results"):
    """
    Save Stan fit, summary dataframe, and input model data in a folder.

    Parameters:
    - stan_fit: Stan fit object
        The Stan fit object obtained after sampling.
    - summary_df: pd.DataFrame
        Summary dataframe obtained from the fit object.
    - input_model_data: dict
        Input model data to be saved.
    - file_name_prefix: str, optional
        Prefix for the folder and file names. Default is "stan_results".

    Returns:
    - dict
        Dictionary containing the file paths of the saved objects.
    """
    # Generate a unique timestamp
    unique_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Get the current working directory
    notebook_directory = os.getcwd()

    # Create a folder with the specified file_name_prefix in the current working directory
    folder_name = f"{file_name_prefix}_{unique_timestamp}"
    folder_path = os.path.join("../", folder_name)
    os.makedirs(folder_path)

    # Save the Stan fit object
    fit_file_path = os.path.join(folder_path, f"{file_name_prefix}_fit_{unique_timestamp}.pkl")
    with open(fit_file_path, "wb") as file:
        pickle.dump(stan_fit, file)

    # Save the summary of the fit object
    summary_file_path = os.path.join(folder_path, f"{file_name_prefix}_summary_{unique_timestamp}.csv")
    summary_df.to_csv(summary_file_path, index=False)

    return {
        "stan_fit_object": fit_file_path,
        "summary_df": summary_file_path,
        "folder_path": folder_path
    }